{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降和随机梯度下降\n",
    "\n",
    "在本节中，我们将介绍梯度下降（gradient descent）的工作原理。虽然梯度下降在深度学习中很少被直接使用，但理解梯度的意义以及沿着梯度反方向更新自变量可能降低目标函数值的原因是学习后续优化算法的基础。随后，我们将引出随机梯度下降（stochastic gradient descent）。\n",
    "\n",
    "## 一维梯度下降\n",
    "\n",
    "我们先以简单的一维梯度下降为例，解释梯度下降算法可能降低目标函数值的原因。假设连续可导的函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$的输入和输出都是标量。给定绝对值足够小的数$\\epsilon$，根据泰勒展开公式（参见附录中[“数学基础”](../chapter_appendix/math.ipynb)一节），我们得到以下的近似：\n",
    "\n",
    "$$f(x + \\epsilon) \\approx f(x) + \\epsilon f'(x) .$$\n",
    "\n",
    "这里$f'(x)$是函数$f$在$x$处的梯度。一维函数的梯度是一个标量，也称导数。\n",
    "\n",
    "接下来，找到一个常数$\\eta > 0$，使得$\\left|\\eta f'(x)\\right|$足够小，那么可以将$\\epsilon$替换为$-\\eta f'(x)$并得到\n",
    "\n",
    "$$f(x - \\eta f'(x)) \\approx f(x) -  \\eta f'(x)^2.$$\n",
    "\n",
    "如果导数$f'(x) \\neq 0$，那么$\\eta f'(x)^2>0$，所以\n",
    "\n",
    "$$f(x - \\eta f'(x)) \\lesssim f(x).$$\n",
    "\n",
    "这意味着，如果通过\n",
    "\n",
    "$$x \\leftarrow x - \\eta f'(x)$$\n",
    "\n",
    "来迭代$x$，函数$f(x)$的值可能会降低。因此在梯度下降中，我们先选取一个初始值$x$和常数$\\eta > 0$，然后不断通过上式来迭代$x$，直到达到停止条件，例如$f'(x)^2$的值已足够小或迭代次数已达到某个值。\n",
    "\n",
    "下面我们以目标函数$f(x)=x^2$为例来看一看梯度下降是如何工作的。虽然我们知道最小化$f(x)$的解为$x=0$，这里依然使用这个简单函数来观察$x$是如何被迭代的。首先，导入本节实验所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "impo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
