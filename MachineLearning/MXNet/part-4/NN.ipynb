{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构造\n",
    "\n",
    "让我们回顾一下在[“多层感知机的简洁实现”](../chapter_deep-learning-basics/mlp-gluon.ipynb)一节中含单隐藏层的多层感知机的实现方法。我们首先构造`Sequential`实例，然后依次添加两个全连接层。其中第一层的输出大小为256，即隐藏层单元个数是256；第二层的输出大小为10，即输出层单元个数是10。我们在上一章的其他\n",
    "节中也使用了`Sequential`类构造模型。这里我们介绍另外一种基于`Block`类的模型构造方法：它让模型构造更加灵活。\n",
    "\n",
    "\n",
    "## 继承`Block`类来构造模型\n",
    "\n",
    "`Block`类是`nn`模块里提供的一个模型构造类，我们可以继承它来定义我们想要的模型。下面继承`Block`类构造本节开头提到的多层感知机。这里定义的`MLP`类重载了`Block`类的`__init__`函数和`forward`函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class MLP(nn.Block):\n",
    "    # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')\n",
    "        self.output = nn.Dense(10)\n",
    "    # 定义前向传播\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的`MLP`类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的`backward`函数。\n",
    "\n",
    "我们可以实例化`MLP`类得到模型变量`net`。下面的代码初始化`net`并传入输入数据`X`做一次前向计算。其中，`net(X)`会调用`MLP`继承自`Block`类的`__call__`函数，这个函数将调用`MLP`类定义的`forward`函数来完成前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.09543003  0.04614331 -0.00286653 -0.07790348 -0.0513024   0.02942039\n",
       "   0.08696644 -0.01907929 -0.04122178  0.05088577]\n",
       " [ 0.07692869  0.03099705  0.00856576 -0.044672   -0.06926841  0.09132433\n",
       "   0.06786595 -0.06187843 -0.03436673  0.04234697]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(2, 20))\n",
    "net = MLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里并没有将`Block`类命名为`Layer`（层）或者`Model`（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如Gluon提供的`Dense`类），又可以是一个模型（如这里定义的`MLP`类），或者是模型的一个部分。我们下面通过两个例子来展示它的灵活性。\n",
    "\n",
    "## `Sequential`类继承自`Block`类\n",
    "\n",
    "我们刚刚提到，`Block`类是一个通用的部件。事实上，`Sequential`类继承自`Block`类。当模型的前向计算为简单串联各个层的计算时，可以通过更加简单的方式定义模型。这正是`Sequential`类的目的：它提供`add`函数来逐一添加串联的`Block`子类实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。\n",
    "\n",
    "下面我们实现一个与`Sequential`类有相同功能的`MySequential`类。这或许可以帮助读者更加清晰地理解`Sequential`类的工作机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MySequential, self).__init__(**kwargs)\n",
    "    \n",
    "    def add(self, block):\n",
    "        self._children[block.name] = block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self._children.values():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00362229  0.00633331  0.03201145 -0.01369376  0.10336448 -0.03508019\n",
       "  -0.00032164 -0.01676024  0.06978627  0.01303309]\n",
       " [ 0.03871719  0.02608211  0.03544958 -0.02521311  0.11005434 -0.01430662\n",
       "  -0.03052466 -0.03852825  0.06321152  0.00385941]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以观察到这里`MySequential`类的使用跟[“多层感知机的简洁实现”](../chapter_deep-learning-basics/mlp-gluon.ipynb)一节中`Sequential`类的使用没什么区别。\n",
    "\n",
    "\n",
    "## 构造复杂的模型\n",
    "\n",
    "虽然`Sequential`类可以使模型构造更加简单，且不需要定义`forward`函数，但直接继承`Block`类可以极大地拓展模型构造的灵活性。下面我们构造一个稍微复杂点的网络`FancyMLP`。在这个网络中，我们通过`get_constant`函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用`NDArray`的函数和Python的控制流，并多次调用相同的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        # get_content创建的随机权重参数不会在训练中被迭代（即常数参数）\n",
    "        self.rand_weight = self.params.get_constant(\n",
    "            'rand_weight', nd.random.uniform(shape=(20, 20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        # 使用创建的常数参数，\n",
    "        x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)\n",
    "        # 复用全连接层\n",
    "        x = self.dense(x)\n",
    "        # 控制流 调用asscalar函数来返回标量进行比较\n",
    "        while x.norm().asscalar() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().asscalar() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[18.571953]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[24.866209]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'),\n",
    "                    nn.Dense(32, activation='relu'))\n",
    "#         self.net = [nn.Dense(64, activation='relu'),\n",
    "#                     nn.Dense(32, activation='relu')]\n",
    "        self.dense = nn.Dense(16, activation='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense(self.net(x))\n",
    "    \n",
    "net = nn.Sequential()\n",
    "net.add(NestMLP(), nn.Dense(20), FancyMLP())\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense10_ (\n",
      "  Parameter dense10_weight (shape=(256, 0), dtype=float32)\n",
      "  Parameter dense10_bias (shape=(256,), dtype=float32)\n",
      ") <class 'mxnet.gluon.parameter.ParameterDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.06281953  0.02262797 -0.04495925 -0.01636342  0.01223227 -0.02480634\n",
       "  -0.03506396 -0.05295732 -0.04518762 -0.09792529]\n",
       " [ 0.01386758 -0.02968637  0.01167846  0.01244943 -0.0305337  -0.02907137\n",
       "  -0.04247145 -0.02945884 -0.02781031 -0.05699408]]\n",
       "<NDArray 2x10 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()  # 默认初始化方式\n",
    "\n",
    "print(net[0].params, type(net[0].params))\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "Y = net(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense10_ (\n",
       "   Parameter dense10_weight (shape=(256, 20), dtype=float32)\n",
       "   Parameter dense10_bias (shape=(256,), dtype=float32)\n",
       " ),\n",
       " mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params, type(net[0].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter dense10_weight (shape=(256, 20), dtype=float32),\n",
       " Parameter dense10_weight (shape=(256, 20), dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params['dense10_weight'], net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.06046963  0.00624272 -0.03472826 ... -0.01759475  0.0686483\n",
       "  -0.06360765]\n",
       " [-0.01273243 -0.02659053 -0.04718638 ...  0.02570021  0.02275064\n",
       "  -0.0166979 ]\n",
       " [-0.03555115  0.01875034  0.02322027 ...  0.06564643  0.04601197\n",
       "  -0.01915742]\n",
       " ...\n",
       " [ 0.03173313  0.01789995  0.02519771 ... -0.06176154 -0.03986754\n",
       "  -0.04898471]\n",
       " [ 0.00564718  0.04665586 -0.00028374 ...  0.05332779  0.02100175\n",
       "  -0.06427249]\n",
       " [ 0.0438781   0.05357236  0.02753124 ...  0.04084889 -0.01963295\n",
       "   0.05668835]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " ...\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 0. ... 0. 0. 0.]]\n",
       "<NDArray 256x20 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 10 @cpu(0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential2_ (\n",
       "  Parameter dense10_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense10_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense11_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense11_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential2_ (\n",
       "  Parameter dense10_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense11_weight (shape=(10, 256), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params('.*weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-0.08032348  0.17926483  0.0174623   0.10047356 -0.01771725  0.1704121\n",
       " -0.03151958 -0.08446401  0.04394737  0.03826509 -0.07153405 -0.1518173\n",
       " -0.01800668  0.15418535  0.04158759 -0.09354302  0.0476378  -0.03466791\n",
       "  0.0468796   0.1868755 ]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import init\n",
    "net.initialize(init=init.Normal(sigma=0.1), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize(init=init.Constant(1), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-0.10389185  0.07822403 -0.1289716  -0.1410463  -0.07610903 -0.10696874\n",
       " -0.01996909 -0.07058676  0.00648634  0.10942626  0.08052795 -0.09453681\n",
       "  0.13527533 -0.01967503 -0.11284603 -0.05156991 -0.11588816  0.02459455\n",
       "  0.02644953  0.12870744]\n",
       "<NDArray 20 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
       "<NDArray 8 @cpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "net(X)\n",
    "\n",
    "net[1].weight.data()[0] == net[2].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义层\n",
    "\n",
    "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然Gluon提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何使用`NDArray`来自定义一个Gluon的层，从而可以被重复调用。\n",
    "\n",
    "\n",
    "## 不含模型参数的自定义层\n",
    "\n",
    "我们先介绍如何定义一个不含模型参数的自定义层。事实上，这和[“模型构造”](model-construction.ipynb)一节中介绍的使用`Block`类构造模型类似。下面的`CenteredLayer`类通过继承`Block`类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了`forward`函数里。这个层里不含模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class CenteredLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x - x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以实例化这个层，然后做前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[-2. -1.  0.  1.  2.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(nd.array([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以用它来构造更复杂的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(128),\n",
    "       CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面打印自定义层各个输出的均值。因为均值是浮点数，所以它的值是一个很接近0的数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.693881e-10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "y = net(nd.random.uniform(shape=(4, 8)))\n",
    "y.mean().asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 含模型参数的自定义层\n",
    "\n",
    "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。\n",
    "\n",
    "[“模型参数的访问、初始化和共享”](parameters.ipynb)一节分别介绍了`Parameter`类和`ParameterDict`类。在自定义含模型参数的层时，我们可以利用`Block`类自带的`ParameterDict`类型的成员变量`params`。它是一个由字符串类型的参数名字映射到Parameter类型的模型参数的字典。我们可以通过`get`函数从`ParameterDict`创建`Parameter`实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  Parameter param2 (shape=(2, 3), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = gluon.ParameterDict()\n",
    "params.get('param2', shape=(2, 3))\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们尝试实现一个含权重参数和偏差参数的全连接层。它使用ReLU函数作为激活函数。其中in_units和units分别代表输入个数和输出个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(nn.Block):\n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(in_units, units))\n",
    "        self.bias = self.params.get('bias', shape=(units, ))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        linear = nd.dot(x, self.weight.data()) + self.bias.data()\n",
    "        return nd.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mydense0_ (\n",
       "  Parameter mydense0_weight (shape=(5, 3), dtype=<class 'numpy.float32'>)\n",
       "  Parameter mydense0_bias (shape=(3,), dtype=<class 'numpy.float32'>)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = MyDense(units=3, in_units=5)\n",
    "dense.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0.]\n",
       " [0. 0. 0.]]\n",
       "<NDArray 2x3 @cpu(0)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(nd.random.uniform(shape=(2, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以直接使用自定义层做前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.]\n",
       " [0.]]\n",
       "<NDArray 2x1 @cpu(0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(MyDense(8, in_units=64),\n",
    "        MyDense(1, in_units=8))\n",
    "net.initialize()\n",
    "net(nd.random.uniform(shape=(2, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
