{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 丢弃法\n",
    "\n",
    "设丢弃概率为p，那么有p的概率hi会被清零，有1 - p的概率hi会除以 1 - p做拉伸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i^{'} = \\frac{\\xi_i}{1 - p} h_i$$\n",
    "$变量\\xi为0和1的概率分别为p和1-p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    if keep_prob == 0: # 全部丢弃\n",
    "        return X.zeros_like()  # keep_prob的概率小于 即为1  drop_prob的概率大于为0\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, 8))\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  4.  6.  0.  0.  0. 14.]\n",
       " [ 0. 18.  0.  0. 24. 26. 28.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "w1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(shape=(num_hiddens1))\n",
    "w2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(shape=(num_hiddens2))\n",
    "w3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(shape=(num_outputs))\n",
    "params = [w1, b1, w2, b2, w3, b3]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "def net(X):\n",
    "    X = X.reshape(-1, num_inputs)\n",
    "    H1 = (nd.dot(X, w1) + b1).relu()\n",
    "    if autograd.is_training():  # 只在训练的时候使用丢弃法\n",
    "        H1 = dropout(H1, drop_prob1)\n",
    "    H2 = (nd.dot(H1, w2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return nd.dot(H2, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2301, train acc 0.524, test acc 0.762\n",
      "epoch 2, loss 0.6027, train acc 0.775, test acc 0.812\n",
      "epoch 3, loss 0.5059, train acc 0.814, test acc 0.843\n",
      "epoch 4, loss 0.4584, train acc 0.833, test acc 0.860\n",
      "epoch 5, loss 0.4290, train acc 0.843, test acc 0.866\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 5, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_without_dropout = nn.Sequential()\n",
    "net_without_dropout.add(nn.Dense(256, activation='relu'),\n",
    "        nn.Dense(256, activation='relu'),\n",
    "        nn.Dense(10))\n",
    "net_without_dropout.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1916, train acc 0.537, test acc 0.574\n",
      "epoch 2, loss 0.5931, train acc 0.778, test acc 0.832\n",
      "epoch 3, loss 0.4646, train acc 0.827, test acc 0.852\n",
      "epoch 4, loss 0.4201, train acc 0.843, test acc 0.862\n",
      "epoch 5, loss 0.3877, train acc 0.857, test acc 0.859\n",
      "epoch 6, loss 0.3651, train acc 0.865, test acc 0.870\n",
      "epoch 7, loss 0.3495, train acc 0.871, test acc 0.872\n",
      "epoch 8, loss 0.3334, train acc 0.875, test acc 0.869\n",
      "epoch 9, loss 0.3217, train acc 0.879, test acc 0.882\n",
      "epoch 10, loss 0.3116, train acc 0.883, test acc 0.881\n",
      "epoch 11, loss 0.3045, train acc 0.887, test acc 0.882\n",
      "epoch 12, loss 0.2932, train acc 0.891, test acc 0.888\n",
      "epoch 13, loss 0.2852, train acc 0.893, test acc 0.887\n",
      "epoch 14, loss 0.2789, train acc 0.895, test acc 0.881\n",
      "epoch 15, loss 0.2780, train acc 0.897, test acc 0.882\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net_without_dropout.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net_without_dropout, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1319, train acc 0.557, test acc 0.764\n",
      "epoch 2, loss 0.5959, train acc 0.774, test acc 0.829\n",
      "epoch 3, loss 0.5090, train acc 0.810, test acc 0.846\n",
      "epoch 4, loss 0.4621, train acc 0.830, test acc 0.839\n",
      "epoch 5, loss 0.4418, train acc 0.836, test acc 0.860\n",
      "epoch 6, loss 0.4212, train acc 0.845, test acc 0.866\n",
      "epoch 7, loss 0.4090, train acc 0.850, test acc 0.866\n",
      "epoch 8, loss 0.3955, train acc 0.854, test acc 0.871\n",
      "epoch 9, loss 0.3834, train acc 0.859, test acc 0.869\n",
      "epoch 10, loss 0.3765, train acc 0.861, test acc 0.873\n",
      "epoch 11, loss 0.3651, train acc 0.865, test acc 0.879\n",
      "epoch 12, loss 0.3618, train acc 0.866, test acc 0.877\n",
      "epoch 13, loss 0.3532, train acc 0.869, test acc 0.880\n",
      "epoch 14, loss 0.3484, train acc 0.870, test acc 0.882\n",
      "epoch 15, loss 0.3430, train acc 0.872, test acc 0.883\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
